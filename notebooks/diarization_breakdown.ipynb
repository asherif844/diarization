{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This script tests individual functions of the diarization function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import sklearn.cluster\n",
    "import scipy\n",
    "import os,json\n",
    "from pyAudioAnalysis import ShortTermFeatures as sF\n",
    "from pyAudioAnalysis import MidTermFeatures as aF\n",
    "from pyAudioAnalysis import audioTrainTest as aT\n",
    "from pyAudioAnalysis import audioBasicIO\n",
    "from scipy.spatial import distance\n",
    "from pyAudioAnalysis import audioSegmentation as aS\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.discriminant_analysis\n",
    "import csv\n",
    "import os.path\n",
    "import sklearn\n",
    "import sklearn.cluster\n",
    "import hmmlearn.hmm\n",
    "import pickle as cPickle\n",
    "import glob\n",
    "from pydub import AudioSegment\n",
    "from pydub.utils import make_chunks\n",
    "from datetime import datetime\n",
    "import pprint\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import diarization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" General utility functions \"\"\"\n",
    "\n",
    "from pyAudioAnalysis.audioSegmentation import (smoothMovingAvg,\n",
    "                                               selfSimilarityMatrix,\n",
    "                                               flags2segs,\n",
    "                                               segs2flags,\n",
    "                                               computePreRec,\n",
    "                                               readSegmentGT,\n",
    "                                               plotSegmentationResults,\n",
    "                                               evaluateSpeakerDiarization,\n",
    "                                               trainHMM_computeStatistics,\n",
    "                                               trainHMM_fromFile,\n",
    "                                               trainHMM_fromDir,\n",
    "                                               hmmSegmentation,\n",
    "                                               mtFileClassification,\n",
    "                                               evaluateSegmentationClassificationDir,\n",
    "                                               silenceRemoval,\n",
    "                                               speakerDiarizationEvaluateScript,\n",
    "                                               musicThumbnailing\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./pyAudioAnalysis/data/Greenway/\n",
      "./pyAudioAnalysis/data/Greenway/audio_test_min_1.wav\n",
      "./pyAudioAnalysis/data/Greenway/audio_test_min_2.wav\n"
     ]
    }
   ],
   "source": [
    "from os.path import sep, join\n",
    "\n",
    "def pjoin(*args, **kwargs):\n",
    "  return join(*args, **kwargs).replace(sep, '/')\n",
    "\n",
    "audio_folder=\".\\\\pyAudioAnalysis\\\\data\\\\Greenway\\\\\"\n",
    "print(pjoin(audio_folder) )\n",
    "types = (pjoin(audio_folder)  + '*.wav',)  # the tuple of file types\n",
    "\n",
    "#Create files list, exclude \"snippet\" files\n",
    "files_list = []\n",
    "for files in glob.glob(audio_folder+\"*.wav\"):\n",
    "    if (\"snippet\" not in files) & (\"_min_\" in files):\n",
    "        files_list.append(pjoin(files))\n",
    "        print(pjoin(files))\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./pyAudioAnalysis/data/Greenway/audio_test_min_1.wav',\n",
       " './pyAudioAnalysis/data/Greenway/audio_test_min_2.wav']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filename=\"./pyAudioAnalysis/data/Greenway/audio_test_min_1.wav\"\n",
    "filename=files_list[0]\n",
    "\n",
    "filename_only=filename if \"/\" not in filename else filename.split(\"/\")[-1]\n",
    "# output_folder=os.getcwd()\n",
    "nameoffile= filename_only.split(\"_min_\")[0]\n",
    "timeoffile=filename_only.split(\"_min_\")[1].replace(\".wav\",\"\")\n",
    "output_folder=\".\\\\pyAudioAnalysis\\\\data\\\\Greenway\\\\\"\n",
    "speech_key=\"52fe944f29784ae288482e5eb3092e2a\"\n",
    "service_region=\"eastus2\"\n",
    "n_speakers=2\n",
    "mt_size=2.0\n",
    "mt_step=0.2\n",
    "st_win=0.05\n",
    "lda_dim=35\n",
    "plot_res=1\n",
    "save_plot=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call GreenwayDiarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GreenwayHealth import fileGreenwaySpeakerDiarization,dirGreenwaySpeakerDiarization\n",
    "# fileGreenwaySpeakerDiarization(filename=filename,output_folder=output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./pyAudioAnalysis/data/Greenway\\audio_test_min_1.wav\n",
      "./pyAudioAnalysis/data/Greenway\\audio_test_min_2.wav\n",
      "./pyAudioAnalysis/data/Greenway\\Conversation.wav\n",
      "./pyAudioAnalysis/data/Greenway\\PhysicianPatientConversation.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anfrankl\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:466: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(26, 6 - 1) = 5 components.\n",
      "  ChangedBehaviorWarning)\n",
      "C:\\Users\\anfrankl\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:472: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
      "  warnings.warn(future_msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Saturday, or next to meet you right pleasure.\n",
      "So you haven't been here in awhile.\n",
      "Open busy.\n",
      "Well, that's that's probably good.\n",
      "Tell me if I'm correct or not, but I just looked through your chart real quick. It looks like you're on no medications correct no medical problems that we know of today.\n",
      "So tell me what you're in why did you come in today?\n",
      "CLOSING on SpeechRecognitionCanceledEventArgs(session_id=273df72aa97c483ebea35bb8ca350237, result=SpeechRecognitionResult(result_id=15e83ce7e79e4030820a3154630eaae8, text=\"\", reason=ResultReason.Canceled))\n",
      "CLOSING on SessionEventArgs(session_id=273df72aa97c483ebea35bb8ca350237)\n",
      "So I decided to rearrange my whole garden with all coming up so I pulled a lot of my beds and was replacing some mums and after that, you know. Of course, or as usual, but what's your favorite flower.\n",
      "This season, I like lilies, but right now, it's mom's because of the winter very nice very nice but I was noticing. You know after a few days. It was just a lot of pain like radiating almost down the side of my hip into my leg and its sharp at certain points, where if I move, One Direction Or.\n",
      "CLOSING on SpeechRecognitionCanceledEventArgs(session_id=0fb6e133b45248cd94471fd1a1ee9c8c, result=SpeechRecognitionResult(result_id=2e9a240e17d545f9b50cf341b4227a80, text=\"\", reason=ResultReason.Canceled))\n",
      "CLOSING on SessionEventArgs(session_id=0fb6e133b45248cd94471fd1a1ee9c8c)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anfrankl\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:466: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(26, 6 - 1) = 5 components.\n",
      "  ChangedBehaviorWarning)\n",
      "C:\\Users\\anfrankl\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:472: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
      "  warnings.warn(future_msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Saturday, or next to meet you right pleasure.\n",
      "So you haven't been here in awhile.\n",
      "Open busy.\n",
      "Well, that's that's probably good.\n",
      "Tell me if I'm correct or not, but I just looked through your chart real quick. It looks like you're on no medications correct no medical problems that we know of today.\n",
      "So tell me what you're in why did you come in today?\n",
      "CLOSING on SpeechRecognitionCanceledEventArgs(session_id=aa6163e51d5e4f38967516e843fd3f97, result=SpeechRecognitionResult(result_id=6d7de6494b5b4cd8b5904fd809ce741a, text=\"\", reason=ResultReason.Canceled))\n",
      "CLOSING on SessionEventArgs(session_id=aa6163e51d5e4f38967516e843fd3f97)\n",
      "So I decided to rearrange my whole garden with all coming up so I pulled a lot of my beds and was replacing some mums and after that, you know. Of course, or as usual, but what's your favorite flower.\n",
      "This season, I like lilies, but right now, it's mom's because of the winter very nice very nice but I was noticing. You know after a few days. It was just a lot of pain like radiating almost down the side of my hip into my leg and its sharp at certain points, where if I move, One Direction Or.\n",
      "CLOSING on SpeechRecognitionCanceledEventArgs(session_id=9dbb729759634fe89c645cf26845c692, result=SpeechRecognitionResult(result_id=3ccff4b654c742b7b5f871fcd58dfb68, text=\"\", reason=ResultReason.Canceled))\n",
      "CLOSING on SessionEventArgs(session_id=9dbb729759634fe89c645cf26845c692)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-0f13123c4b9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdirGreenwaySpeakerDiarization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudio_folder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\".\\\\pyAudioAnalysis\\\\data\\\\Greenway\\\\\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput_folder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"./pyAudioAnalysis/data/Greenway/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\pyAudioAnalysis\\GreenwayHealth.py\u001b[0m in \u001b[0;36mdirGreenwaySpeakerDiarization\u001b[1;34m(audio_folder, output_folder)\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;31m#Call Greenway Diarization Function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mwav_file\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfiles_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mfileGreenwaySpeakerDiarization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwav_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput_folder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_folder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m def fileGreenwaySpeakerDiarization(filename,output_folder,speech_key=\"52fe944f29784ae288482e5eb3092e2a\",service_region=\"eastus2\",\\\n",
      "\u001b[1;32m~\\Desktop\\pyAudioAnalysis\\GreenwayHealth.py\u001b[0m in \u001b[0;36mfileGreenwaySpeakerDiarization\u001b[1;34m(filename, output_folder, speech_key, service_region, n_speakers, mt_size, mt_step, st_win, lda_dim, plot_res, save_plot)\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[0mfilename_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilename\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;34m\"/\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[0mnameoffile\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mfilename_only\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_min_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m     \u001b[0mtimeoffile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilename_only\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_min_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dirGreenwaySpeakerDiarization(audio_folder=\".\\\\pyAudioAnalysis\\\\data\\\\Greenway\\\\\",output_folder=\"./pyAudioAnalysis/data/Greenway/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anfrankl\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:466: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(26, 6 - 1) = 5 components.\n",
      "  ChangedBehaviorWarning)\n",
      "C:\\Users\\anfrankl\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:472: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
      "  warnings.warn(future_msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Saturday, or next to meet you right pleasure.\n",
      "So you haven't been here in awhile.\n",
      "Open busy.\n",
      "Well, that's that's probably good.\n",
      "Tell me if I'm correct or not, but I just looked through your chart real quick. It looks like you're on no medications correct no medical problems that we know of today.\n",
      "So tell me what you're in why did you come in today?\n",
      "CLOSING on SpeechRecognitionCanceledEventArgs(session_id=15e15d29515a43d58b2c0ad45d28b062, result=SpeechRecognitionResult(result_id=dd33a26d59ee49bb996373c4e15d37b5, text=\"\", reason=ResultReason.Canceled))\n",
      "CLOSING on SessionEventArgs(session_id=15e15d29515a43d58b2c0ad45d28b062)\n",
      "So I decided to rearrange my whole garden with all coming up so I pulled a lot of my beds and was replacing some mums and after that, you know. Of course, or as usual, but what's your favorite flower.\n",
      "This season, I like lilies, but right now, it's mom's because of the winter very nice very nice but I was noticing. You know after a few days. It was just a lot of pain like radiating almost down the side of my hip into my leg and its sharp at certain points, where if I move, One Direction Or.\n",
      "CLOSING on SpeechRecognitionCanceledEventArgs(session_id=3f1244213d6b48588059320a71a06b6d, result=SpeechRecognitionResult(result_id=0bb83a61b43446939d23c7833af1ad52, text=\"\", reason=ResultReason.Canceled))\n",
      "CLOSING on SessionEventArgs(session_id=3f1244213d6b48588059320a71a06b6d)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anfrankl\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:466: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(26, 6 - 1) = 5 components.\n",
      "  ChangedBehaviorWarning)\n",
      "C:\\Users\\anfrankl\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:472: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
      "  warnings.warn(future_msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Saturday, or next to meet you right pleasure.\n",
      "So you haven't been here in awhile.\n",
      "Open busy.\n",
      "Well, that's that's probably good.\n",
      "Tell me if I'm correct or not, but I just looked through your chart real quick. It looks like you're on no medications correct no medical problems that we know of today.\n",
      "So tell me what you're in why did you come in today?\n",
      "CLOSING on SpeechRecognitionCanceledEventArgs(session_id=eb8ea0d0338d4870b00142287c2ed2d4, result=SpeechRecognitionResult(result_id=a4ade52a4c134f76b5d61f705c7c10a5, text=\"\", reason=ResultReason.Canceled))\n",
      "CLOSING on SessionEventArgs(session_id=eb8ea0d0338d4870b00142287c2ed2d4)\n",
      "So I decided to rearrange my whole garden with all coming up so I pulled a lot of my beds and was replacing some mums and after that, you know. Of course, or as usual, but what's your favorite flower.\n",
      "This season, I like lilies, but right now, it's mom's because of the winter very nice very nice but I was noticing. You know after a few days. It was just a lot of pain like radiating almost down the side of my hip into my leg and its sharp at certain points, where if I move, One Direction Or.\n",
      "CLOSING on SpeechRecognitionCanceledEventArgs(session_id=96a9c812fe8a4e10bf332dfc89d48079, result=SpeechRecognitionResult(result_id=80a3e15231494e1591d7f085bcb6adf0, text=\"\", reason=ResultReason.Canceled))\n",
      "CLOSING on SessionEventArgs(session_id=96a9c812fe8a4e10bf332dfc89d48079)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for wav_file in files_list:\n",
    "        fileGreenwaySpeakerDiarization(filename=pjoin(wav_file),output_folder=\".\\\\pyAudioAnalysis\\\\data\\\\Greenway\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'audio_test_min_1.wav'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'audio_test'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nameoffile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeoffile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.\\\\pyAudioAnalysis\\\\data\\\\Greenway\\\\'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./pyAudioAnalysis/data/Greenway/audio_test_min_1.wav'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "[fs, x] = audioBasicIO.read_audio_file(filename)\n",
    "x = audioBasicIO.stereo_to_mono(x)\n",
    "duration = len(x) / fs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59.989333333333335"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segment audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "[classifier_1, MEAN1, STD1, classNames1, mtWin1, mtStep1, stWin1, stStep1, computeBEAT1] = aT.load_model_knn(os.path.join(pathname, \"data\\\\models\", \"knn_speaker_10\"))\n",
    "[classifier_2, MEAN2, STD2, classNames2, mtWin2, mtStep2, stWin2, stStep2, computeBEAT2] = aT.load_model_knn(os.path.join(pathname, \"data\\\\models\", \"knn_speaker_male_female\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Midterm features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "[mt_feats, st_feats, _] = aF.mid_feature_extraction(x, fs, mt_size * fs,\n",
    "                                                    mt_step * fs,\n",
    "                                                    round(fs * st_win),\n",
    "                                                    round(fs*st_win * 0.5))\n",
    "\n",
    "MidTermFeatures2 = np.zeros((mt_feats.shape[0] + len(classNames1) +\n",
    "                                len(classNames2), mt_feats.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(mt_feats.shape[1]):\n",
    "    cur_f1 = (mt_feats[:, i] - MEAN1) / STD1\n",
    "    cur_f2 = (mt_feats[:, i] - MEAN2) / STD2\n",
    "    [res, P1] = aT.classifierWrapper(classifier_1, \"knn\", cur_f1)\n",
    "    [res, P2] = aT.classifierWrapper(classifier_2, \"knn\", cur_f2)\n",
    "    MidTermFeatures2[0:mt_feats.shape[0], i] = mt_feats[:, i]\n",
    "    MidTermFeatures2[mt_feats.shape[0]:mt_feats.shape[0]+len(classNames1), i] = P1 + 0.0001\n",
    "    MidTermFeatures2[mt_feats.shape[0] + len(classNames1)::, i] = P2 + 0.0001\n",
    "\n",
    "mt_feats = MidTermFeatures2    # TODO\n",
    "iFeaturesSelect = [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 41,\n",
    "                   42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53]\n",
    "\n",
    "mt_feats = mt_feats[iFeaturesSelect, :]\n",
    "\n",
    "(mt_feats_norm, MEAN, STD) = aT.normalizeFeatures([mt_feats.T])\n",
    "mt_feats_norm = mt_feats_norm[0].T\n",
    "n_wins = mt_feats.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers:\n",
    "dist_all = np.sum(distance.squareform(distance.pdist(mt_feats_norm.T)),\n",
    "                     axis=0)\n",
    "m_dist_all = np.mean(dist_all)\n",
    "i_non_outliers = np.nonzero(dist_all < 1.2 * m_dist_all)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "perOutLier = (100.0 * (n_wins - i_non_outliers.shape[0])) / n_wins\n",
    "mt_feats_norm_or = mt_feats_norm\n",
    "mt_feats_norm = mt_feats_norm[:, i_non_outliers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anfrankl\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:466: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(26, 6 - 1) = 5 components.\n",
      "  ChangedBehaviorWarning)\n",
      "C:\\Users\\anfrankl\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:472: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
      "  warnings.warn(future_msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# LDA dimensionality reduction:\n",
    "if lda_dim > 0:\n",
    "    #[mt_feats_to_red, _, _] = aF.mtFeatureExtraction(x, fs, mt_size * fs,\n",
    "    # st_win * fs, round(fs*st_win), round(fs*st_win));\n",
    "    # extract mid-term features with minimum step:\n",
    "    mt_win_ratio = int(round(mt_size / st_win))\n",
    "    mt_step_ratio = int(round(st_win / st_win))\n",
    "    mt_feats_to_red = []\n",
    "    num_of_features = len(st_feats)\n",
    "    num_of_stats = 2\n",
    "    #for i in range(num_of_stats * num_of_features + 1):\n",
    "    for i in range(num_of_stats * num_of_features):\n",
    "        mt_feats_to_red.append([])\n",
    "\n",
    "    for i in range(num_of_features):  # for each of the short-term features:\n",
    "        curPos = 0\n",
    "        N = len(st_feats[i])\n",
    "        while (curPos < N):\n",
    "            N1 = curPos\n",
    "            N2 = curPos + mt_win_ratio\n",
    "            if N2 > N:\n",
    "                N2 = N\n",
    "            curStFeatures = st_feats[i][N1:N2]\n",
    "            mt_feats_to_red[i].append(np.mean(curStFeatures))\n",
    "            mt_feats_to_red[i+num_of_features].append(np.std(curStFeatures))\n",
    "            curPos += mt_step_ratio\n",
    "    mt_feats_to_red = np.array(mt_feats_to_red)\n",
    "    mt_feats_to_red_2 = np.zeros((mt_feats_to_red.shape[0] +\n",
    "                                    len(classNames1) + len(classNames2),\n",
    "                                     mt_feats_to_red.shape[1]))\n",
    "    for i in range(mt_feats_to_red.shape[1]):\n",
    "        cur_f1 = (mt_feats_to_red[:, i] - MEAN1) / STD1\n",
    "        cur_f2 = (mt_feats_to_red[:, i] - MEAN2) / STD2\n",
    "        [res, P1] = aT.classifierWrapper(classifier_1, \"knn\", cur_f1)\n",
    "        [res, P2] = aT.classifierWrapper(classifier_2, \"knn\", cur_f2)\n",
    "        mt_feats_to_red_2[0:mt_feats_to_red.shape[0], i] = mt_feats_to_red[:, i]\n",
    "        mt_feats_to_red_2[mt_feats_to_red.shape[0]:mt_feats_to_red.shape[0] + len(classNames1), i] = P1 + 0.0001\n",
    "        mt_feats_to_red_2[mt_feats_to_red.shape[0]+len(classNames1)::, i] = P2 + 0.0001\n",
    "    mt_feats_to_red = mt_feats_to_red_2\n",
    "    mt_feats_to_red = mt_feats_to_red[iFeaturesSelect, :]\n",
    "    #mt_feats_to_red += np.random.rand(mt_feats_to_red.shape[0], mt_feats_to_red.shape[1]) * 0.0000010\n",
    "    (mt_feats_to_red, MEAN, STD) = aT.normalizeFeatures([mt_feats_to_red.T])\n",
    "    mt_feats_to_red = mt_feats_to_red[0].T\n",
    "    #dist_all = np.sum(distance.squareform(distance.pdist(mt_feats_to_red.T)), axis=0)\n",
    "    #m_dist_all = np.mean(dist_all)\n",
    "    #iNonOutLiers2 = np.nonzero(dist_all < 3.0*m_dist_all)[0]\n",
    "    #mt_feats_to_red = mt_feats_to_red[:, iNonOutLiers2]\n",
    "    Labels = np.zeros((mt_feats_to_red.shape[1], ));\n",
    "    LDAstep = 1.0\n",
    "    LDAstepRatio = LDAstep / st_win\n",
    "    #print LDAstep, LDAstepRatio\n",
    "    for i in range(Labels.shape[0]):\n",
    "        Labels[i] = int(i*st_win/LDAstepRatio);        \n",
    "    clf = sklearn.discriminant_analysis.LinearDiscriminantAnalysis(n_components=lda_dim)\n",
    "    clf.fit(mt_feats_to_red.T, Labels)\n",
    "    mt_feats_norm = (clf.transform(mt_feats_norm.T)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_speakers <= 0:\n",
    "    s_range = range(2, 10)\n",
    "else:\n",
    "    s_range = [n_speakers]\n",
    "clsAll = []\n",
    "sil_all = []\n",
    "centersAll = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iSpeakers in s_range:        \n",
    "    k_means = sklearn.cluster.KMeans(n_clusters=iSpeakers)\n",
    "    k_means.fit(mt_feats_norm.T)\n",
    "    cls = k_means.labels_        \n",
    "    means = k_means.cluster_centers_\n",
    "\n",
    "    # Y = distance.squareform(distance.pdist(mt_feats_norm.T))\n",
    "    clsAll.append(cls)\n",
    "    centersAll.append(means)\n",
    "    sil_1 = []; sil_2 = []\n",
    "    for c in range(iSpeakers):\n",
    "        # for each speaker (i.e. for each extracted cluster)\n",
    "        clust_per_cent = np.nonzero(cls == c)[0].shape[0] / \\\n",
    "                         float(len(cls))\n",
    "        if clust_per_cent < 0.020:\n",
    "            sil_1.append(0.0)\n",
    "            sil_2.append(0.0)\n",
    "        else:\n",
    "            # get subset of feature vectors\n",
    "            mt_feats_norm_temp = mt_feats_norm[:, cls==c]\n",
    "            # compute average distance between samples\n",
    "            # that belong to the cluster (a values)\n",
    "            Yt = distance.pdist(mt_feats_norm_temp.T)\n",
    "            sil_1.append(np.mean(Yt)*clust_per_cent)\n",
    "            silBs = []\n",
    "            for c2 in range(iSpeakers):\n",
    "                # compute distances from samples of other clusters\n",
    "                if c2 != c:\n",
    "                    clust_per_cent_2 = np.nonzero(cls == c2)[0].shape[0] /\\\n",
    "                                       float(len(cls))\n",
    "                    MidTermFeaturesNormTemp2 = mt_feats_norm[:, cls == c2]\n",
    "                    Yt = distance.cdist(mt_feats_norm_temp.T, \n",
    "                                        MidTermFeaturesNormTemp2.T)\n",
    "                    silBs.append(np.mean(Yt)*(clust_per_cent\n",
    "                                                 + clust_per_cent_2)/2.0)\n",
    "            silBs = np.array(silBs)\n",
    "            # ... and keep the minimum value (i.e.\n",
    "            # the distance from the \"nearest\" cluster)\n",
    "            sil_2.append(min(silBs))\n",
    "    sil_1 = np.array(sil_1); \n",
    "    sil_2 = np.array(sil_2); \n",
    "    sil = []\n",
    "    for c in range(iSpeakers):\n",
    "        # for each cluster (speaker) compute silhouette\n",
    "        sil.append( ( sil_2[c] - sil_1[c]) / (max(sil_2[c],\n",
    "                                                  sil_1[c]) + 0.00001))\n",
    "    # keep the AVERAGE SILLOUETTE\n",
    "    sil_all.append(np.mean(sil))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "imax = np.argmax(sil_all)\n",
    "# optimal number of clusters\n",
    "nSpeakersFinal = s_range[imax]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final cluster labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the final set of cluster labels\n",
    "# (important: need to retrieve the outlier windows:\n",
    "# this is achieved by giving them the value of their\n",
    "# nearest non-outlier window)\n",
    "cls = np.zeros((n_wins,))\n",
    "for i in range(n_wins):\n",
    "    j = np.argmin(np.abs(i-i_non_outliers))        \n",
    "    cls[i] = clsAll[imax][j]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Postprocess for smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-process method 1: hmm smoothing\n",
    "for i in range(1):\n",
    "    # hmm training\n",
    "    start_prob, transmat, means, cov = \\\n",
    "        trainHMM_computeStatistics(mt_feats_norm_or, cls)\n",
    "    hmm = hmmlearn.hmm.GaussianHMM(start_prob.shape[0], \"diag\")\n",
    "    hmm.startprob_ = start_prob\n",
    "    hmm.transmat_ = transmat            \n",
    "    hmm.means_ = means; hmm.covars_ = cov\n",
    "    cls = hmm.predict(mt_feats_norm_or.T)                    \n",
    "\n",
    "# Post-process method 2: median filtering:\n",
    "cls = scipy.signal.medfilt(cls, 13)\n",
    "cls = scipy.signal.medfilt(cls, 11)\n",
    "\n",
    "sil = sil_all[imax]\n",
    "class_names = [\"speaker{0:d}\".format(c) for c in range(nSpeakersFinal)];\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Ground Truth if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ground-truth if available\n",
    "gt_file = filename.replace('.wav', '.segments')\n",
    "# if groundturh exists\n",
    "if os.path.isfile(gt_file):\n",
    "    [seg_start, seg_end, seg_labs] = readSegmentGT(gt_file)\n",
    "    flags_gt, class_names_gt = segs2flags(seg_start, seg_end, seg_labs, mt_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Develop Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if plot_res:\n",
    "    fig = plt.figure()    \n",
    "    if n_speakers > 0:\n",
    "        ax1 = fig.add_subplot(111)\n",
    "    else:\n",
    "        ax1 = fig.add_subplot(211)\n",
    "    ax1.set_yticks(np.array(range(len(class_names))))\n",
    "    ax1.axis((0, duration, -1, len(class_names)))\n",
    "    ax1.set_yticklabels(class_names)\n",
    "    ax1.plot(np.array(range(len(cls)))*mt_step+mt_step/2.0, cls)\n",
    "\n",
    "if os.path.isfile(gt_file):\n",
    "    if plot_res:\n",
    "        ax1.plot(np.array(range(len(flags_gt))) *\n",
    "                 mt_step + mt_step / 2.0, flags_gt, 'r')\n",
    "    purity_cluster_m, purity_speaker_m = \\\n",
    "        evaluateSpeakerDiarization(cls, flags_gt)\n",
    "    print(\"{0:.1f}\\t{1:.1f}\".format(100 * purity_cluster_m,\n",
    "                                    100 * purity_speaker_m))\n",
    "    if plot_res:\n",
    "        plt.title(\"Cluster purity: {0:.1f}% - \"\n",
    "                  \"Speaker purity: {1:.1f}%\".format(100 * purity_cluster_m,\n",
    "                                                    100 * purity_speaker_m))\n",
    "if plot_res:\n",
    "    plt.xlabel(\"time (seconds)\")\n",
    "    #print s_range, sil_all    \n",
    "    if n_speakers<=0:\n",
    "        plt.subplot(212)\n",
    "        plt.plot(s_range, sil_all)\n",
    "        plt.xlabel(\"number of clusters\");\n",
    "        plt.ylabel(\"average clustering's sillouette\");\n",
    "    plt.show()\n",
    "\n",
    "if save_plot:\n",
    "    plt.savefig(f\"{output_folder}{filename_only}\".replace(\".wav\",\".png\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create time vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_vec=np.array(range(len(cls)))*mt_step+mt_step/2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.1,  0.3,  0.5,  0.7,  0.9,  1.1,  1.3,  1.5,  1.7,  1.9,  2.1,\n",
       "        2.3,  2.5,  2.7,  2.9,  3.1,  3.3,  3.5,  3.7,  3.9,  4.1,  4.3,\n",
       "        4.5,  4.7,  4.9,  5.1,  5.3,  5.5,  5.7,  5.9,  6.1,  6.3,  6.5,\n",
       "        6.7,  6.9,  7.1,  7.3,  7.5,  7.7,  7.9,  8.1,  8.3,  8.5,  8.7,\n",
       "        8.9,  9.1,  9.3,  9.5,  9.7,  9.9, 10.1, 10.3, 10.5, 10.7, 10.9,\n",
       "       11.1, 11.3, 11.5, 11.7, 11.9, 12.1, 12.3, 12.5, 12.7, 12.9, 13.1,\n",
       "       13.3, 13.5, 13.7, 13.9, 14.1, 14.3, 14.5, 14.7, 14.9, 15.1, 15.3,\n",
       "       15.5, 15.7, 15.9, 16.1, 16.3, 16.5, 16.7, 16.9, 17.1, 17.3, 17.5,\n",
       "       17.7, 17.9, 18.1, 18.3, 18.5, 18.7, 18.9, 19.1, 19.3, 19.5, 19.7,\n",
       "       19.9, 20.1, 20.3, 20.5, 20.7, 20.9, 21.1, 21.3, 21.5, 21.7, 21.9,\n",
       "       22.1, 22.3, 22.5, 22.7, 22.9, 23.1, 23.3, 23.5, 23.7, 23.9, 24.1,\n",
       "       24.3, 24.5, 24.7, 24.9, 25.1, 25.3, 25.5, 25.7, 25.9, 26.1, 26.3,\n",
       "       26.5, 26.7, 26.9, 27.1, 27.3, 27.5, 27.7, 27.9, 28.1, 28.3, 28.5,\n",
       "       28.7, 28.9, 29.1, 29.3, 29.5, 29.7, 29.9, 30.1, 30.3, 30.5, 30.7,\n",
       "       30.9, 31.1, 31.3, 31.5, 31.7, 31.9, 32.1, 32.3, 32.5, 32.7, 32.9,\n",
       "       33.1, 33.3, 33.5, 33.7, 33.9, 34.1, 34.3, 34.5, 34.7, 34.9, 35.1,\n",
       "       35.3, 35.5, 35.7, 35.9, 36.1, 36.3, 36.5, 36.7, 36.9, 37.1, 37.3,\n",
       "       37.5, 37.7, 37.9, 38.1, 38.3, 38.5, 38.7, 38.9, 39.1, 39.3, 39.5,\n",
       "       39.7, 39.9, 40.1, 40.3, 40.5, 40.7, 40.9, 41.1, 41.3, 41.5, 41.7,\n",
       "       41.9, 42.1, 42.3, 42.5, 42.7, 42.9, 43.1, 43.3, 43.5, 43.7, 43.9,\n",
       "       44.1, 44.3, 44.5, 44.7, 44.9, 45.1, 45.3, 45.5, 45.7, 45.9, 46.1,\n",
       "       46.3, 46.5, 46.7, 46.9, 47.1, 47.3, 47.5, 47.7, 47.9, 48.1, 48.3,\n",
       "       48.5, 48.7, 48.9, 49.1, 49.3, 49.5, 49.7, 49.9, 50.1, 50.3, 50.5,\n",
       "       50.7, 50.9, 51.1, 51.3, 51.5, 51.7, 51.9, 52.1, 52.3, 52.5, 52.7,\n",
       "       52.9, 53.1, 53.3, 53.5, 53.7, 53.9, 54.1, 54.3, 54.5, 54.7, 54.9,\n",
       "       55.1, 55.3, 55.5, 55.7, 55.9, 56.1, 56.3, 56.5, 56.7, 56.9, 57.1,\n",
       "       57.3, 57.5, 57.7, 57.9, 58.1, 58.3, 58.5, 58.7, 58.9, 59.1, 59.3,\n",
       "       59.5, 59.7, 59.9])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find Change points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_change_index=np.where(np.roll(cls,1)!=cls)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'dialogue_id': '2020-02-23 02:41:05.243760',\n",
      "  'end_time': 26.300000000000004,\n",
      "  'sequence_id': '0',\n",
      "  'speaker': 0.0,\n",
      "  'start_time': 0.1,\n",
      "  'text': ''},\n",
      " {'dialogue_id': '2020-02-23 02:41:05.244763',\n",
      "  'end_time': 59.900000000000006,\n",
      "  'sequence_id': '1',\n",
      "  'speaker': 1.0,\n",
      "  'start_time': 26.500000000000004,\n",
      "  'text': ''}]\n"
     ]
    }
   ],
   "source": [
    "output_list=[]\n",
    "temp={}\n",
    "for ind,sc in enumerate(speaker_change_index):\n",
    "    temp['dialogue_id']= str(datetime.now()).strip()\n",
    "    temp['sequence_id']=str(ind)\n",
    "    temp['speaker']=list(cls)[sc]\n",
    "    temp['start_time']=time_vec[sc]\n",
    "    temp['end_time']=time_vec[speaker_change_index[ind+1]-1] if ind+1<len(speaker_change_index) else time_vec[-1]\n",
    "    temp[\"text\"]=\"\"\n",
    "    output_list.append(temp)\n",
    "    temp={}\n",
    "    \n",
    "pprint.pprint(output_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wav snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snip_transcribe(output_list,filename,output_folder=output_folder,speech_key=\"52fe944f29784ae288482e5eb3092e2a\",service_region=\"eastus2\"):\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)\n",
    "    speech_config.enable_dictation\n",
    "    \n",
    "    def recognized_cb(evt):\n",
    "        if evt.result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "            # Do something with the recognized text\n",
    "            output_list[ind]['text']=output_list[ind]['text']+str(evt.result.text)\n",
    "            print(evt.result.text)\n",
    "    \n",
    "    for ind,diag in enumerate(output_list):\n",
    "        t1=diag['start_time']\n",
    "        t2=diag['end_time']\n",
    "        newAudio = AudioSegment.from_wav(filename)\n",
    "        chunk = newAudio[t1*1000:t2*1000]\n",
    "        filename_out=output_folder+ f\"snippet_{diag['sequence_id']}.wav\"\n",
    "        chunk.export(filename_out, format=\"wav\") #Exports to a wav file in the current path.\n",
    "        done=False\n",
    "        def stop_cb(evt):\n",
    "            \"\"\"callback that signals to stop continuous recognition upon receiving an event `evt`\"\"\"\n",
    "            print('CLOSING on {}'.format(evt))\n",
    "            nonlocal done\n",
    "            done = True\n",
    "        \n",
    "        audio_input=speechsdk.AudioConfig(filename=filename_out)\n",
    "        speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_input)\n",
    "#         result=speech_recognizer.start_continuous_recognition()\n",
    "        output_list[ind]['snippet_path']=filename_out\n",
    "#         output_list[ind]['text']=result.text\n",
    "                        \n",
    "        speech_recognizer.recognized.connect(recognized_cb)\n",
    "\n",
    "        speech_recognizer.session_stopped.connect(stop_cb)\n",
    "        speech_recognizer.canceled.connect(stop_cb)\n",
    "\n",
    "        # Start continuous speech recognition\n",
    "        speech_recognizer.start_continuous_recognition()\n",
    "        while not done:\n",
    "            time.sleep(.5)\n",
    "\n",
    "        speech_recognizer.stop_continuous_recognition()\n",
    "\n",
    "#         if result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "            \n",
    "#         elif result.reason == speechsdk.ResultReason.NoMatch:\n",
    "#             output_list[ind]['text']=result.no_match_details\n",
    "#         elif result.reason == speechsdk.ResultReason.Canceled:\n",
    "#             cancellation_details = result.cancellation_details\n",
    "#             output_list[ind]['text']=cancellation_details.reason\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Saturday, or next to meet you right pleasure.\n",
      "So you haven't been here in awhile.\n",
      "Open busy.\n",
      "Well, that's that's probably good.\n",
      "Tell me if I'm correct or not, but I just looked through your chart real quick. It looks like you're on no medications correct no medical problems that we know of today.\n",
      "So tell me what you're in why did you come in today?\n",
      "CLOSING on SpeechRecognitionCanceledEventArgs(session_id=cda0a8347c934d56b9abf2d2f4aad75d, result=SpeechRecognitionResult(result_id=59ca81ee97b94c3490679896a3f34e2a, text=\"\", reason=ResultReason.Canceled))\n",
      "CLOSING on SessionEventArgs(session_id=cda0a8347c934d56b9abf2d2f4aad75d)\n",
      "So I decided to rearrange my whole garden with all coming up so I pulled a lot of my beds and was replacing some mums and after that, you know. Of course, or as usual, but what's your favorite flower.\n",
      "This season, I like lilies, but right now, it's mom's because of the winter very nice very nice but I was noticing. You know after a few days. It was just a lot of pain like radiating almost down the side of my hip into my leg and its sharp at certain points, where if I move, One Direction Or.\n",
      "CLOSING on SpeechRecognitionCanceledEventArgs(session_id=feba08f3a1634b49b14f532ebf692b9f, result=SpeechRecognitionResult(result_id=713e85e81ffd401e9ed0c957f78e6df3, text=\"\", reason=ResultReason.Canceled))\n",
      "CLOSING on SessionEventArgs(session_id=feba08f3a1634b49b14f532ebf692b9f)\n"
     ]
    }
   ],
   "source": [
    "output=snip_transcribe(output_list,filepath,output_folder=output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'dialogue_id': '2020-02-22 10:11:55.514182',\n",
       "  'sequence_id': '0',\n",
       "  'speaker': 0.0,\n",
       "  'start_time': 0.1,\n",
       "  'end_time': 26.300000000000004,\n",
       "  'text': \"On Saturday, or next to meet you right pleasure.So you haven't been here in awhile.Open busy.Well, that's that's probably good.Tell me if I'm correct or not, but I just looked through your chart real quick. It looks like you're on no medications correct no medical problems that we know of today.So tell me what you're in why did you come in today?\",\n",
       "  'snippet_path': '.\\\\pyAudioAnalysis\\\\data\\\\Greenway\\\\snippet_0.wav'},\n",
       " {'dialogue_id': '2020-02-22 10:11:55.514182',\n",
       "  'sequence_id': '1',\n",
       "  'speaker': 1.0,\n",
       "  'start_time': 26.500000000000004,\n",
       "  'end_time': 59.900000000000006,\n",
       "  'text': \"So I decided to rearrange my whole garden with all coming up so I pulled a lot of my beds and was replacing some mums and after that, you know. Of course, or as usual, but what's your favorite flower.This season, I like lilies, but right now, it's mom's because of the winter very nice very nice but I was noticing. You know after a few days. It was just a lot of pain like radiating almost down the side of my hip into my leg and its sharp at certain points, where if I move, One Direction Or.\",\n",
       "  'snippet_path': '.\\\\pyAudioAnalysis\\\\data\\\\Greenway\\\\snippet_1.wav'}]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio_test_min_1.wav': [{'dialogue_id': '2020-02-22 10:11:55.514182',\n",
       "   'sequence_id': '0',\n",
       "   'speaker': 0.0,\n",
       "   'start_time': 0.1,\n",
       "   'end_time': 26.300000000000004,\n",
       "   'text': \"On Saturday, or next to meet you right pleasure.So you haven't been here in awhile.Open busy.Well, that's that's probably good.Tell me if I'm correct or not, but I just looked through your chart real quick. It looks like you're on no medications correct no medical problems that we know of today.So tell me what you're in why did you come in today?\",\n",
       "   'snippet_path': '.\\\\pyAudioAnalysis\\\\data\\\\Greenway\\\\snippet_0.wav'},\n",
       "  {'dialogue_id': '2020-02-22 10:11:55.514182',\n",
       "   'sequence_id': '1',\n",
       "   'speaker': 1.0,\n",
       "   'start_time': 26.500000000000004,\n",
       "   'end_time': 59.900000000000006,\n",
       "   'text': \"So I decided to rearrange my whole garden with all coming up so I pulled a lot of my beds and was replacing some mums and after that, you know. Of course, or as usual, but what's your favorite flower.This season, I like lilies, but right now, it's mom's because of the winter very nice very nice but I was noticing. You know after a few days. It was just a lot of pain like radiating almost down the side of my hip into my leg and its sharp at certain points, where if I move, One Direction Or.\",\n",
       "   'snippet_path': '.\\\\pyAudioAnalysis\\\\data\\\\Greenway\\\\snippet_1.wav'}]}"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{filename_only:output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./pyAudioAnalysis/data/Greenway/\n",
      "./pyAudioAnalysis/data/Greenway/audio_test_min_1.wav\n",
      "./pyAudioAnalysis/data/Greenway/Conversation.wav\n",
      "./pyAudioAnalysis/data/Greenway/PhysicianPatientConversation.wav\n"
     ]
    }
   ],
   "source": [
    "from os.path import sep, join\n",
    "\n",
    "def pjoin(*args, **kwargs):\n",
    "  return join(*args, **kwargs).replace(sep, '/')\n",
    "\n",
    "audio_folder=\".\\\\pyAudioAnalysis\\\\data\\\\Greenway\\\\\"\n",
    "print(pjoin(audio_folder) )\n",
    "types = (pjoin(audio_folder)  + '*.wav',)  # the tuple of file types\n",
    "\n",
    "#Create files list, exclude \"snippet\" files\n",
    "files_list = []\n",
    "for files in glob.glob(audio_folder+\"*.wav\"):\n",
    "    if \"snippet\" not in files:\n",
    "        files_list.append(pjoin(files))\n",
    "        print(pjoin(files))\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./pyAudioAnalysis/data/Greenway/audio_test_min_1.wav',\n",
       " './pyAudioAnalysis/data/Greenway/Conversation.wav',\n",
       " './pyAudioAnalysis/data/Greenway/PhysicianPatientConversation.wav']"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GreenwayHealth' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-244-0832c02f48a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGreenwayHealth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirGreenwaySpeakerDiarization\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'GreenwayHealth' is not defined"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(GreenwayHealth.dirGreenwaySpeakerDiarization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GreenwayHealth import dirGreenwaySpeakerDiarization,fileGreenwaySpeakerDiarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./pyAudioAnalysis/data/Greenway/audio_test_min_1.wav\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\anfrankl\\\\Desktop\\\\pyAudioAnalysis\\\\data/models\\\\knn_speaker_10'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-233-f928cf587bbc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mwav_file\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfiles_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwav_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mfileGreenwaySpeakerDiarization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwav_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput_folder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_folder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\pyAudioAnalysis\\GreenwayHealth.py\u001b[0m in \u001b[0;36mfileGreenwaySpeakerDiarization\u001b[1;34m(filename, output_folder, speech_key, service_region, n_speakers, mt_size, mt_step, st_win, lda_dim, plot_res, save_plot)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m     \u001b[1;33m[\u001b[0m\u001b[0mfs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maudioBasicIO\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_audio_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maudioBasicIO\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstereo_to_mono\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[0mduration\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mfs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\pyAudioAnalysis\\pyAudioAnalysis\\audioTrainTest.py\u001b[0m in \u001b[0;36mload_model_knn\u001b[1;34m(kNNModelName, is_regression)\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_model_knn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkNNModelName\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_regression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 578\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkNNModelName\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    579\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m         \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\anfrankl\\\\Desktop\\\\pyAudioAnalysis\\\\data/models\\\\knn_speaker_10'"
     ]
    }
   ],
   "source": [
    "#Call Greenway Diarization Function\n",
    "for wav_file in files_list:\n",
    "    print(wav_file)\n",
    "    fileGreenwaySpeakerDiarization(filename=wav_file,output_folder=output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirGreenwaySpeakerDiarization(audio_folder=\".\\\\pyAudioAnalysis\\\\data\\\\Greenway\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transcribe the wav snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_key, service_region = \"52fe944f29784ae288482e5eb3092e2a\",\"eastus2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "snippet_list=[]\n",
    "for name in glob.glob(output_folder+'snippet_?.wav'):\n",
    "    snippet_list.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.\\\\pyAudioAnalysis\\\\data\\\\Greenway\\\\snippet_0.wav',\n",
       " '.\\\\pyAudioAnalysis\\\\data\\\\Greenway\\\\snippet_1.wav']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(snippet_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for x in snippet_list:\n",
    "    print(x.replace(\".wav\",\"\").split(\"snippet_\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognizing first result...\n",
      "Recognized: On Saturday, or next to meet you pleasure.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Creates an audio configuration that points to an audio file.\n",
    "# Replace with your own audio filename.\n",
    "# audio_filename = \"whatstheweatherlike.wav\"\n",
    "audio_input = speechsdk.AudioConfig(filename=\"./chunk0.wav\")\n",
    "\n",
    "# Creates a recognizer with the given settings\n",
    "speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_input)\n",
    "\n",
    "print(\"Recognizing first result...\")\n",
    "\n",
    "# Starts speech recognition, and returns after a single utterance is recognized. The end of a\n",
    "# single utterance is determined by listening for silence at the end or until a maximum of 15\n",
    "# seconds of audio is processed.  The task returns the recognition text as result. \n",
    "# Note: Since recognize_once() returns only a single utterance, it is suitable only for single\n",
    "# shot recognition like command or query. \n",
    "# For long-running multi-utterance recognition, use start_continuous_recognition() instead.\n",
    "result = speech_recognizer.recognize_once()\n",
    "\n",
    "# Checks result.\n",
    "if result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "    print(\"Recognized: {}\".format(result.text))\n",
    "elif result.reason == speechsdk.ResultReason.NoMatch:\n",
    "    print(\"No speech could be recognized: {}\".format(result.no_match_details))\n",
    "elif result.reason == speechsdk.ResultReason.Canceled:\n",
    "    cancellation_details = result.cancellation_details\n",
    "    print(\"Speech Recognition canceled: {}\".format(cancellation_details.reason))\n",
    "    if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "        print(\"Error details: {}\".format(cancellation_details.error_details))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Creates an audio configuration that points to an audio file.\n",
    "# Replace with your own audio filename.\n",
    "# audio_filename = \"whatstheweatherlike.wav\"\n",
    "audio_input = speechsdk.AudioConfig(filename=\"./chunk0.wav\")\n",
    "\n",
    "# Creates a recognizer with the given settings\n",
    "speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_input)\n",
    "\n",
    "print(\"Recognizing first result...\")\n",
    "\n",
    "# Starts speech recognition, and returns after a single utterance is recognized. The end of a\n",
    "# single utterance is determined by listening for silence at the end or until a maximum of 15\n",
    "# seconds of audio is processed.  The task returns the recognition text as result. \n",
    "# Note: Since recognize_once() returns only a single utterance, it is suitable only for single\n",
    "# shot recognition like command or query. \n",
    "# For long-running multi-utterance recognition, use start_continuous_recognition() instead.\n",
    "done = False\n",
    "\n",
    "def stop_cb(evt):\n",
    "    \"\"\"callback that signals to stop continuous recognition upon receiving an event `evt`\"\"\"\n",
    "    print('CLOSING on {}'.format(evt))\n",
    "    done = True\n",
    "\n",
    "# Connect callbacks to the events fired by the speech recognizer\n",
    "# speech_recognizer.recognizing.connect(lambda evt: print('RECOGNIZING: {}'.format(evt)))\n",
    "speech_recognizer.recognized.connect(lambda evt: print('RECOGNIZED: {}'.format(evt)))\n",
    "# speech_recognizer.session_started.connect(lambda evt: print('SESSION STARTED: {}'.format(evt)))\n",
    "# speech_recognizer.session_stopped.connect(lambda evt: print('SESSION STOPPED {}'.format(evt)))\n",
    "# speech_recognizer.canceled.connect(lambda evt: print('CANCELED {}'.format(evt)))\n",
    "# # stop continuous recognition on either session stopped or canceled events\n",
    "speech_recognizer.session_stopped.connect(stop_cb)\n",
    "speech_recognizer.canceled.connect(stop_cb)\n",
    "\n",
    "# Start continuous speech recognition\n",
    "speech_recognizer.start_continuous_recognition()\n",
    "while not done:\n",
    "    time.sleep(.5)\n",
    "\n",
    "speech_recognizer.stop_continuous_recognition()\n",
    "# </SpeechContinuousRecognitionWithFile>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import logging\n",
    "import sys\n",
    "import requests\n",
    "import time\n",
    "import swagger_client as cris_client\n",
    "\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG, format=\"%(message)s\")\n",
    "\n",
    "# Your subscription key and region for the speech service\n",
    "SUBSCRIPTION_KEY = speech_key\n",
    "SERVICE_REGION = service_region\n",
    "\n",
    "NAME = \"Simple transcription\"\n",
    "DESCRIPTION = \"Simple transcription description\"\n",
    "\n",
    "LOCALE = \"en-US\"\n",
    "RECORDINGS_BLOB_URI = \"https://ahmedblob2019.blob.core.windows.net/audio/Tim%20knee%20urgent%20care.wav?sp=rcwd&st=2020-02-18T17:56:34Z&se=2030-02-19T01:56:34Z&spr=https&sv=2019-02-02&sr=b&sig=AZ3a5hWPFW2j6Jsr2NKA%2BBgPoYtC74VYnDhRbF%2FvjOI%3D\"\n",
    "\n",
    "# Set subscription information when doing transcription with custom models\n",
    "ADAPTED_ACOUSTIC_ID = None  # guid of a custom acoustic model\n",
    "ADAPTED_LANGUAGE_ID = None  # guid of a custom language model\n",
    "\n",
    "\n",
    "def transcribe():\n",
    "    logging.info(\"Starting transcription client...\")\n",
    "\n",
    "    # configure API key authorization: subscription_key\n",
    "    configuration = cris_client.Configuration()\n",
    "    configuration.api_key['Ocp-Apim-Subscription-Key'] = SUBSCRIPTION_KEY\n",
    "    configuration.host = \"https://{}.cris.ai\".format(SERVICE_REGION)\n",
    "\n",
    "    # create the client object and authenticate\n",
    "    client = cris_client.ApiClient(configuration)\n",
    "\n",
    "    # create an instance of the transcription api class\n",
    "    transcription_api = cris_client.CustomSpeechTranscriptionsApi(api_client=client)\n",
    "\n",
    "    # Specify transcription properties by passing a dict to the properties parameter. See\n",
    "    # https://docs.microsoft.com/azure/cognitive-services/speech-service/batch-transcription#configuration-properties\n",
    "    # for supported parameters.\n",
    "    properties = {\n",
    "        # 'PunctuationMode': 'DictatedAndAutomatic',\n",
    "        # 'ProfanityFilterMode': 'Masked',\n",
    "        'AddWordLevelTimestamps': 'True',\n",
    "        'AddDiarization': 'True',\n",
    "        # 'AddSentiment': False,\n",
    "        # 'TranscriptionResultsContainerUrl': \"<results container>\"\n",
    "    }\n",
    "\n",
    "    # Use base models for transcription. Comment this block if you are using a custom model.\n",
    "    transcription_definition = cris_client.TranscriptionDefinition(\n",
    "        name=NAME, description=DESCRIPTION, locale=LOCALE, recordings_url=RECORDINGS_BLOB_URI,\n",
    "        properties=properties\n",
    "    )\n",
    "\n",
    "    # Uncomment this block to use custom models for transcription.\n",
    "    # Model information (ADAPTED_ACOUSTIC_ID and ADAPTED_LANGUAGE_ID) must be set above.\n",
    "    # if ADAPTED_ACOUSTIC_ID is None or ADAPTED_LANGUAGE_ID is None:\n",
    "    #     logging.info(\"Custom model ids must be set to when using custom models\")\n",
    "    # transcription_definition = cris_client.TranscriptionDefinition(\n",
    "    #     name=NAME, description=DESCRIPTION, locale=LOCALE, recordings_url=RECORDINGS_BLOB_URI,\n",
    "    #     models=[cris_client.ModelIdentity(ADAPTED_ACOUSTIC_ID), cris_client.ModelIdentity(ADAPTED_LANGUAGE_ID)],\n",
    "    #     properties=properties\n",
    "    # )\n",
    "\n",
    "    data, status, headers = transcription_api.create_transcription_with_http_info(transcription_definition)\n",
    "\n",
    "    # extract transcription location from the headers\n",
    "    transcription_location: str = headers[\"location\"]\n",
    "\n",
    "    # get the transcription Id from the location URI\n",
    "    created_transcription: str = transcription_location.split('/')[-1]\n",
    "\n",
    "    logging.info(\"Created new transcription with id {}\".format(created_transcription))\n",
    "\n",
    "    logging.info(\"Checking status.\")\n",
    "\n",
    "    completed = False\n",
    "\n",
    "    while not completed:\n",
    "        running, not_started = 0, 0\n",
    "\n",
    "        # get all transcriptions for the user\n",
    "        transcriptions: List[cris_client.Transcription] = transcription_api.get_transcriptions()\n",
    "\n",
    "        # for each transcription in the list we check the status\n",
    "        for transcription in transcriptions:\n",
    "            if transcription.status in (\"Failed\", \"Succeeded\"):\n",
    "                # we check to see if it was the transcription we created from this client\n",
    "                if created_transcription != transcription.id:\n",
    "                    continue\n",
    "\n",
    "                completed = True\n",
    "\n",
    "                if transcription.status == \"Succeeded\":\n",
    "                    results_uri = transcription.results_urls[\"channel_0\"]\n",
    "                    results = requests.get(results_uri)\n",
    "                    logging.info(\"Transcription succeeded. Results: \")\n",
    "                    logging.info(results.content.decode(\"utf-8\"))\n",
    "                else:\n",
    "                    logging.info(\"Transcription failed :{}.\".format(transcription.status_message))\n",
    "                    break\n",
    "            elif transcription.status == \"Running\":\n",
    "                running += 1\n",
    "            elif transcription.status == \"NotStarted\":\n",
    "                not_started += 1\n",
    "\n",
    "        logging.info(\"Transcriptions status: \"\n",
    "                \"completed (this transcription): {}, {} running, {} not started yet\".format(\n",
    "                    completed, running, not_started))\n",
    "\n",
    "        # wait for 5 seconds\n",
    "        time.sleep(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=transcribe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
