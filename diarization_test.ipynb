{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This script tests individual functions of the diarization function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import sklearn.cluster\n",
    "import scipy\n",
    "import os\n",
    "from pyAudioAnalysis import ShortTermFeatures as sF\n",
    "from pyAudioAnalysis import MidTermFeatures as aF\n",
    "from pyAudioAnalysis import audioTrainTest as aT\n",
    "from pyAudioAnalysis import audioBasicIO\n",
    "from scipy.spatial import distance\n",
    "from pyAudioAnalysis import audioSegmentation as aS\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.discriminant_analysis\n",
    "import csv\n",
    "import os.path\n",
    "import sklearn\n",
    "import sklearn.cluster\n",
    "import hmmlearn.hmm\n",
    "import pickle as cPickle\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import diarization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" General utility functions \"\"\"\n",
    "\n",
    "from pyAudioAnalysis.audioSegmentation import (smoothMovingAvg,\n",
    "                                               selfSimilarityMatrix,\n",
    "                                               flags2segs,\n",
    "                                               segs2flags,\n",
    "                                               computePreRec,\n",
    "                                               readSegmentGT,\n",
    "                                               plotSegmentationResults,\n",
    "                                               evaluateSpeakerDiarization,\n",
    "                                               trainHMM_computeStatistics,\n",
    "                                               trainHMM_fromFile,\n",
    "                                               trainHMM_fromDir,\n",
    "                                               hmmSegmentation,\n",
    "                                               mtFileClassification,\n",
    "                                               evaluateSegmentationClassificationDir,\n",
    "                                               silenceRemoval,\n",
    "                                               speakerDiarizationEvaluateScript,\n",
    "                                               musicThumbnailing\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename=\"./pyAudioAnalysis/data/Greenway/Conversation.wav\"\n",
    "# filename=\"./pyAudioAnalysis/data/Greenway/PhysicianPatientConversation.wav\"\n",
    "filename=\"./pyAudioAnalysis/data/Greenway/test_1_min.wav\"\n",
    "n_speakers=2\n",
    "mt_size=2.0\n",
    "mt_step=0.2\n",
    "st_win=0.05\n",
    "lda_dim=35\n",
    "plot_res=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pathname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathname='C:\\\\Users\\\\anfrankl\\\\Desktop\\\\pyAudioAnalysis\\\\pyAudioAnalysis'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "[fs, x] = audioBasicIO.read_audio_file(filename)\n",
    "x = audioBasicIO.stereo_to_mono(x)\n",
    "duration = len(x) / fs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59.989333333333335"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segment audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "[classifier_1, MEAN1, STD1, classNames1, mtWin1, mtStep1, stWin1, stStep1, computeBEAT1] = aT.load_model_knn(os.path.join(pathname, \"data\\\\models\", \"knn_speaker_10\"))\n",
    "[classifier_2, MEAN2, STD2, classNames2, mtWin2, mtStep2, stWin2, stStep2, computeBEAT2] = aT.load_model_knn(os.path.join(pathname, \"data\\\\models\", \"knn_speaker_male_female\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Midterm features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "[mt_feats, st_feats, _] = aF.mid_feature_extraction(x, fs, mt_size * fs,\n",
    "                                                    mt_step * fs,\n",
    "                                                    round(fs * st_win),\n",
    "                                                    round(fs*st_win * 0.5))\n",
    "\n",
    "MidTermFeatures2 = np.zeros((mt_feats.shape[0] + len(classNames1) +\n",
    "                                len(classNames2), mt_feats.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(mt_feats.shape[1]):\n",
    "    cur_f1 = (mt_feats[:, i] - MEAN1) / STD1\n",
    "    cur_f2 = (mt_feats[:, i] - MEAN2) / STD2\n",
    "    [res, P1] = aT.classifierWrapper(classifier_1, \"knn\", cur_f1)\n",
    "    [res, P2] = aT.classifierWrapper(classifier_2, \"knn\", cur_f2)\n",
    "    MidTermFeatures2[0:mt_feats.shape[0], i] = mt_feats[:, i]\n",
    "    MidTermFeatures2[mt_feats.shape[0]:mt_feats.shape[0]+len(classNames1), i] = P1 + 0.0001\n",
    "    MidTermFeatures2[mt_feats.shape[0] + len(classNames1)::, i] = P2 + 0.0001\n",
    "\n",
    "mt_feats = MidTermFeatures2    # TODO\n",
    "iFeaturesSelect = [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 41,\n",
    "                   42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53]\n",
    "\n",
    "mt_feats = mt_feats[iFeaturesSelect, :]\n",
    "\n",
    "(mt_feats_norm, MEAN, STD) = aT.normalizeFeatures([mt_feats.T])\n",
    "mt_feats_norm = mt_feats_norm[0].T\n",
    "n_wins = mt_feats.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers:\n",
    "dist_all = np.sum(distance.squareform(distance.pdist(mt_feats_norm.T)),\n",
    "                     axis=0)\n",
    "m_dist_all = np.mean(dist_all)\n",
    "i_non_outliers = np.nonzero(dist_all < 1.2 * m_dist_all)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "perOutLier = (100.0 * (n_wins - i_non_outliers.shape[0])) / n_wins\n",
    "mt_feats_norm_or = mt_feats_norm\n",
    "mt_feats_norm = mt_feats_norm[:, i_non_outliers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anfrankl\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:466: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(26, 6 - 1) = 5 components.\n",
      "  ChangedBehaviorWarning)\n",
      "C:\\Users\\anfrankl\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:472: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
      "  warnings.warn(future_msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# LDA dimensionality reduction:\n",
    "if lda_dim > 0:\n",
    "    #[mt_feats_to_red, _, _] = aF.mtFeatureExtraction(x, fs, mt_size * fs,\n",
    "    # st_win * fs, round(fs*st_win), round(fs*st_win));\n",
    "    # extract mid-term features with minimum step:\n",
    "    mt_win_ratio = int(round(mt_size / st_win))\n",
    "    mt_step_ratio = int(round(st_win / st_win))\n",
    "    mt_feats_to_red = []\n",
    "    num_of_features = len(st_feats)\n",
    "    num_of_stats = 2\n",
    "    #for i in range(num_of_stats * num_of_features + 1):\n",
    "    for i in range(num_of_stats * num_of_features):\n",
    "        mt_feats_to_red.append([])\n",
    "\n",
    "    for i in range(num_of_features):  # for each of the short-term features:\n",
    "        curPos = 0\n",
    "        N = len(st_feats[i])\n",
    "        while (curPos < N):\n",
    "            N1 = curPos\n",
    "            N2 = curPos + mt_win_ratio\n",
    "            if N2 > N:\n",
    "                N2 = N\n",
    "            curStFeatures = st_feats[i][N1:N2]\n",
    "            mt_feats_to_red[i].append(np.mean(curStFeatures))\n",
    "            mt_feats_to_red[i+num_of_features].append(np.std(curStFeatures))\n",
    "            curPos += mt_step_ratio\n",
    "    mt_feats_to_red = np.array(mt_feats_to_red)\n",
    "    mt_feats_to_red_2 = np.zeros((mt_feats_to_red.shape[0] +\n",
    "                                    len(classNames1) + len(classNames2),\n",
    "                                     mt_feats_to_red.shape[1]))\n",
    "    for i in range(mt_feats_to_red.shape[1]):\n",
    "        cur_f1 = (mt_feats_to_red[:, i] - MEAN1) / STD1\n",
    "        cur_f2 = (mt_feats_to_red[:, i] - MEAN2) / STD2\n",
    "        [res, P1] = aT.classifierWrapper(classifier_1, \"knn\", cur_f1)\n",
    "        [res, P2] = aT.classifierWrapper(classifier_2, \"knn\", cur_f2)\n",
    "        mt_feats_to_red_2[0:mt_feats_to_red.shape[0], i] = mt_feats_to_red[:, i]\n",
    "        mt_feats_to_red_2[mt_feats_to_red.shape[0]:mt_feats_to_red.shape[0] + len(classNames1), i] = P1 + 0.0001\n",
    "        mt_feats_to_red_2[mt_feats_to_red.shape[0]+len(classNames1)::, i] = P2 + 0.0001\n",
    "    mt_feats_to_red = mt_feats_to_red_2\n",
    "    mt_feats_to_red = mt_feats_to_red[iFeaturesSelect, :]\n",
    "    #mt_feats_to_red += np.random.rand(mt_feats_to_red.shape[0], mt_feats_to_red.shape[1]) * 0.0000010\n",
    "    (mt_feats_to_red, MEAN, STD) = aT.normalizeFeatures([mt_feats_to_red.T])\n",
    "    mt_feats_to_red = mt_feats_to_red[0].T\n",
    "    #dist_all = np.sum(distance.squareform(distance.pdist(mt_feats_to_red.T)), axis=0)\n",
    "    #m_dist_all = np.mean(dist_all)\n",
    "    #iNonOutLiers2 = np.nonzero(dist_all < 3.0*m_dist_all)[0]\n",
    "    #mt_feats_to_red = mt_feats_to_red[:, iNonOutLiers2]\n",
    "    Labels = np.zeros((mt_feats_to_red.shape[1], ));\n",
    "    LDAstep = 1.0\n",
    "    LDAstepRatio = LDAstep / st_win\n",
    "    #print LDAstep, LDAstepRatio\n",
    "    for i in range(Labels.shape[0]):\n",
    "        Labels[i] = int(i*st_win/LDAstepRatio);        \n",
    "    clf = sklearn.discriminant_analysis.LinearDiscriminantAnalysis(n_components=lda_dim)\n",
    "    clf.fit(mt_feats_to_red.T, Labels)\n",
    "    mt_feats_norm = (clf.transform(mt_feats_norm.T)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_speakers <= 0:\n",
    "    s_range = range(2, 10)\n",
    "else:\n",
    "    s_range = [n_speakers]\n",
    "clsAll = []\n",
    "sil_all = []\n",
    "centersAll = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iSpeakers in s_range:        \n",
    "    k_means = sklearn.cluster.KMeans(n_clusters=iSpeakers)\n",
    "    k_means.fit(mt_feats_norm.T)\n",
    "    cls = k_means.labels_        \n",
    "    means = k_means.cluster_centers_\n",
    "\n",
    "    # Y = distance.squareform(distance.pdist(mt_feats_norm.T))\n",
    "    clsAll.append(cls)\n",
    "    centersAll.append(means)\n",
    "    sil_1 = []; sil_2 = []\n",
    "    for c in range(iSpeakers):\n",
    "        # for each speaker (i.e. for each extracted cluster)\n",
    "        clust_per_cent = np.nonzero(cls == c)[0].shape[0] / \\\n",
    "                         float(len(cls))\n",
    "        if clust_per_cent < 0.020:\n",
    "            sil_1.append(0.0)\n",
    "            sil_2.append(0.0)\n",
    "        else:\n",
    "            # get subset of feature vectors\n",
    "            mt_feats_norm_temp = mt_feats_norm[:, cls==c]\n",
    "            # compute average distance between samples\n",
    "            # that belong to the cluster (a values)\n",
    "            Yt = distance.pdist(mt_feats_norm_temp.T)\n",
    "            sil_1.append(np.mean(Yt)*clust_per_cent)\n",
    "            silBs = []\n",
    "            for c2 in range(iSpeakers):\n",
    "                # compute distances from samples of other clusters\n",
    "                if c2 != c:\n",
    "                    clust_per_cent_2 = np.nonzero(cls == c2)[0].shape[0] /\\\n",
    "                                       float(len(cls))\n",
    "                    MidTermFeaturesNormTemp2 = mt_feats_norm[:, cls == c2]\n",
    "                    Yt = distance.cdist(mt_feats_norm_temp.T, \n",
    "                                        MidTermFeaturesNormTemp2.T)\n",
    "                    silBs.append(np.mean(Yt)*(clust_per_cent\n",
    "                                                 + clust_per_cent_2)/2.0)\n",
    "            silBs = np.array(silBs)\n",
    "            # ... and keep the minimum value (i.e.\n",
    "            # the distance from the \"nearest\" cluster)\n",
    "            sil_2.append(min(silBs))\n",
    "    sil_1 = np.array(sil_1); \n",
    "    sil_2 = np.array(sil_2); \n",
    "    sil = []\n",
    "    for c in range(iSpeakers):\n",
    "        # for each cluster (speaker) compute silhouette\n",
    "        sil.append( ( sil_2[c] - sil_1[c]) / (max(sil_2[c],\n",
    "                                                  sil_1[c]) + 0.00001))\n",
    "    # keep the AVERAGE SILLOUETTE\n",
    "    sil_all.append(np.mean(sil))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "imax = np.argmax(sil_all)\n",
    "# optimal number of clusters\n",
    "nSpeakersFinal = s_range[imax]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nSpeakersFinal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final cluster labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the final set of cluster labels\n",
    "# (important: need to retrieve the outlier windows:\n",
    "# this is achieved by giving them the value of their\n",
    "# nearest non-outlier window)\n",
    "cls = np.zeros((n_wins,))\n",
    "for i in range(n_wins):\n",
    "    j = np.argmin(np.abs(i-i_non_outliers))        \n",
    "    cls[i] = clsAll[imax][j]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Postprocess for smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-process method 1: hmm smoothing\n",
    "for i in range(1):\n",
    "    # hmm training\n",
    "    start_prob, transmat, means, cov = \\\n",
    "        trainHMM_computeStatistics(mt_feats_norm_or, cls)\n",
    "    hmm = hmmlearn.hmm.GaussianHMM(start_prob.shape[0], \"diag\")\n",
    "    hmm.startprob_ = start_prob\n",
    "    hmm.transmat_ = transmat            \n",
    "    hmm.means_ = means; hmm.covars_ = cov\n",
    "    cls = hmm.predict(mt_feats_norm_or.T)                    \n",
    "\n",
    "# Post-process method 2: median filtering:\n",
    "cls = scipy.signal.medfilt(cls, 13)\n",
    "cls = scipy.signal.medfilt(cls, 11)\n",
    "\n",
    "sil = sil_all[imax]\n",
    "class_names = [\"speaker{0:d}\".format(c) for c in range(nSpeakersFinal)];\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Ground Truth if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ground-truth if available\n",
    "gt_file = filename.replace('.wav', '.segments')\n",
    "# if groundturh exists\n",
    "if os.path.isfile(gt_file):\n",
    "    [seg_start, seg_end, seg_labs] = readSegmentGT(gt_file)\n",
    "    flags_gt, class_names_gt = segs2flags(seg_start, seg_end, seg_labs, mt_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./pyAudioAnalysis/data/Greenway/test_1_min.wav'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.isfile(gt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./pyAudioAnalysis/data/Greenway/test_1_min.segments'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Develop Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEGCAYAAABPdROvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAARdElEQVR4nO3dfbBdVX3G8e+TBBEVQwWsAqYRhFFEjXBFLdYpliIFrKK1Um0V7QzF+oa2tVrf31AURYtWh2lVHCsItVqwVlALgqmCieVF5cUMaEUcMRUFhcLce3/94+ybewxJbrJXbg459/uZyZy9195nn7Vu5tznrr32XjtVhSRJfS0adQUkSds3g0SS1MQgkSQ1MUgkSU0MEklSkyWjrsC2tttuu9Xy5ctHXQ1J2q6sXr16bVXtvqFtCy5Ili9fzqpVq0ZdDUnariT5wca2eWpLktTEIJEkNTFIJElNDBJJUhODRJLUxCCRJDUxSCRJTQwSSVITg0SS1MQgkSQ1MUgkSU0MEklSE4NEktTEIJEkNTFIJElNDBJJUhODRJLUxCCRJDUxSCRJTQwSSVITg0SS1MQgkSQ1MUgkSU0MEklSE4NEktTEIJEkNTFIJElNDBJJUhODRJLUxCCRJDUxSCRJTQwSSVITg0SS1MQgkSQ1MUgkSU0MEklSE4NEktTEIJEkNTFIJElNDBJJUhODRJLUxCCRJDUxSCRJTQwSSVITg0SS1MQgkSQ1MUgkSU0MEklSE4NEktTEIJEkNTFIJElNDBJJUhODRJLUxCCRJDUxSCRJTQwSSVITg0SS1MQgkSQ1MUgkSU0MEklSE4NEktTEIJEkNTFIJElNDBJJUhODRJLUxCCRJDUxSCRJTQwSSVKTkQZJkuOSfLDh/bsmuTDJL1uOI0nqb8moK9BXkiXA/wFvAA7o/kmStrE5gyTJfYGzgb2AxcDbgJOBTwOHdrs9t6rWJNkd+AiwrCs/sapWJjkYeD+wE3AH8MKquna9zzkKeD3wNCAbOc6bgT2A5cDaqnou8LUkD+vRdqnJF676MW869ztU1airIo3U5vRIjgBuqqqjAJIsZRAkt1bVwUmezyAkjgY+AJxaVV9Lsgw4H3gEcA3w5KqaTHIYcBLwrJkPSHIM8CrgyKq6JcmnNnIcgIOAJ1XVHZvbyCTHA8cDLFu2bI69pc1zxQ9/zs9+dRfHPu4ho66KNO9Wb2Lb5gTJVcApSU4GPl9VlyQBOLPbfiZward8GLB/tx3g/kl2BpYCZyTZFyhgh6HjHwpMAIdX1a1zHAfg3C0JEYCqOh04HWBiYsI/H7VVTE4XO+2wmHcc86hRV0WadydtYtucQVJV1yU5CDgSeGeSC2Y2De/WvS4Cnrj+L/okpwEXVtUxSZYDFw1tvh7YG9gPWDXHcQB+NVedpW1harpYvChz7yiNuTmv2kqyB3B7VX0SOAU4sNv0nKHXr3fLFwAvHXrvim5xKfCjbvm49T7iB8AzgU8keeQcx5HuMSanp1likEibdfnvo4DLklwOvA54e1e+Y5JLgVcAr+zKXg5MJLkyyXeBE7rydzPozaxkMGD/a7qB9+cB5yTZZxPHuZsk3wfeBxyX5MYk+29Gm6Rm9kikgfS54qT75T1RVWu3eo3m2cTERK1atWruHaU5/M05V7ByzVr+67W/N+qqSPMuyeqqmtjQNu9sl3qami4WL7ZHIvW6IbGqlm/lekjbncnpYski/xaT/BZIPTlGIg0YJFJPXrUlDRgkUk/2SKQBg0TqaTBGYpBIBonU09R0scggkQwSqa8peyQSYJBIvU06RiIBBonU25T3kUiAQSL1Zo9EGjBIpJ6mvI9EAgwSqbfJKXskEhgkUm9T08USJ22UDBKpr8Gd7X6FJL8FUk/e2S4NGCRST861JQ0YJFJPzv4rDRgkUk/2SKQBg0TqyTESacAgkXqamvKqLQkMEqm3Se8jkQCDROrNMRJpwCCRevKqLWnAIJF6qCqmCxbFIJEMEqmHqekCsEciYZBIvUx2QbLYwXbJIJH6sEcizTJIpB7W9Ui8j0QySKQ+7JFIswwSqYfJ6WkA7yORMEikXuyRSLMMEqmHyamZMRKDRDJIpB7W9Ui8/FcySKQ+vGpLmuW3QOrBMRJplkEi9eBVW9Isg0TqwR6JNMsgkXqYHSMxSCSDROphtkfiV0jyWyD14H0k0iyDROrB+0ikWQaJ1MPMVVs+IVEySKRepsurtqQZBonUg2Mk0iyDROrBMRJplkEi9TDpDYnSOgaJ1MOUkzZK6/gtkHqwRyLNMkikHqactFFaxyCRerBHIs0ySKQeppy0UVrHIJF6mLmPxEkbJYNE6mVdj8T7SCSDROrDMRJplkEi9eBVW9Isg0TqYd0TEp39VzJIpD6mpotFgUX2SCSDROpjcrq8Ykvq+E2QepiaLsdHpI5BIvUwOWWQSDMMEqmH6TJIpBkGidTD5PS095BInZEGSZLjknyw8RivTbImybVJnrq16iZtimMk0qwlo65AX0mWAPsBxwKPBPYAvpxkv6qaGmnlNPYmp8oeidSZM0iS3Bc4G9gLWAy8DTgZ+DRwaLfbc6tqTZLdgY8Ay7ryE6tqZZKDgfcDOwF3AC+sqmvX+5yjgNcDTwOykeO8mUFgLAfWAlcBZ1XVncANSdYABwNf31h7brn9Lj628oa5mi1t0nU/uc15tqTO5vRIjgBuqqqjAJIsZRAkt1bVwUmezyAkjgY+AJxaVV9Lsgw4H3gEcA3w5KqaTHIYcBLwrJkPSHIM8CrgyKq6JcmnNnIcgIOAJ1XVHd1psW8M1fVGYM/1G5DkeOB4gJ0etA9vOe+7m/XDkTbliXvvOuoqSPcImxMkVwGnJDkZ+HxVXZLBtBBndtvPBE7tlg8D9s/stBH3T7IzsBQ4I8m+QAE7DB3/UGACOLyqbp3jOADnVtUd3fKG/iSsuxVUnQ6cDvDYAw+qC9/4+5vRbGnT7rfjdntmWNqq5vwmVNV1SQ4CjgTemeSCmU3Du3Wvi4AnDv2iByDJacCFVXVMkuXARUObrwf2ZjDesWqO4wD8aqjoRuAhQ+t7ATdtqj2LF4Vd7nOvTe0iSdoCc161lWQP4Paq+iRwCnBgt+k5Q68zYxIXAC8deu+KbnEp8KNu+bj1PuIHwDOBTyR55BzHWd+5wLFJdkzyUGBf4LK52iRJ2no25/LfRwGXJbkceB3w9q58xySXAq8AXtmVvRyYSHJlku8CJ3Tl72bQm1nJYMD+13QD788DzkmyzyaOs/77vsPgQoDvAl8EXuIVW5K0baXqbkMKc78p+T4wUVVrt3qN5tnExEStWrVq7h0lSeskWV1VExva5p3tkqQmvS47qarlW7kekqTtlD0SSVITg0SS1MQgkSQ1MUgkSU0MEklSE4NEktTEIJEkNTFIJElNDBJJUhODRJLUxCCRJDUxSCRJTQwSSVITg0SS1MQgkSQ1MUgkSU0MEklSE4NEktTEIJEkNTFIJElNDBJJUhODRJLUxCCRJDUxSCRJTQwSSVITg0SS1MQgkSQ1MUgkSU0MEklSE4NEktTEIJEkNTFIJElNDBJJUhODRJLUxCCRJDUxSCRJTQwSSVITg0SS1MQgkSQ1MUgkSU0MEklSE4NEktTEIJEkNTFIJElNDBJJUhODRJLUxCCRJDUxSCRJTQwSSVITg0SS1MQgkSQ1MUgkSU0MEklSE4NEktTEIJEkNTFIJElNDBJJUhODRJLUxCCRJDUxSCRJTQwSSVITg0SS1MQgkSQ1MUgkSU0MEklSk1TVqOuwTSW5Dbh21PWYR7sBa0ddiXlk+7Zv49y+cW4bwG9V1e4b2rBkW9fkHuDaqpoYdSXmS5JVtm/7Zfu2X+Pctrl4akuS1MQgkSQ1WYhBcvqoKzDPbN/2zfZtv8a5bZu04AbbJUlb10LskUiStiKDRJLUZEEFSZIjklybZE2S14y6Pq2SfDTJzUm+PVT2gCRfSvK97vU3RlnHvpI8JMmFSa5O8p0kr+jKx6V9905yWZIruva9pSsfi/bNSLI4yX8n+Xy3PjbtS/L9JFcluTzJqq5sbNq3JRZMkCRZDHwI+ANgf+BPkuw/2lo1+zhwxHplrwG+UlX7Al/p1rdHk8BfVdUjgCcAL+n+v8alfXcCT6mqxwArgCOSPIHxad+MVwBXD62PW/sOraoVQ/ePjFv7NsuCCRLgYGBNVV1fVXcBZwFPH3GdmlTVxcDP1it+OnBGt3wG8IxtWqmtpKp+XFXf6pZvY/DLaE/Gp31VVb/sVnfo/hVj0j6AJHsBRwH/OFQ8Nu3biHFv3wYtpCDZE/jh0PqNXdm4+c2q+jEMfhkDDxxxfZolWQ48FriUMWpfd9rncuBm4EtVNVbtA94PvBqYHiobp/YVcEGS1UmO78rGqX2bbSFNkZINlHnt8z1ckvsBnwFOrKpbkw39N26fqmoKWJFkF+CzSQ4YdZ22liRHAzdX1eokvzvq+syTQ6rqpiQPBL6U5JpRV2hUFlKP5EbgIUPrewE3jagu8+knSR4M0L3ePOL69JZkBwYh8s9V9a9d8di0b0ZV/Ry4iMF417i07xDgD5N8n8Fp5Kck+STj0z6q6qbu9WbgswxOn49N+7bEQgqSbwL7JnloknsBxwLnjrhO8+Fc4AXd8guAfxthXXrLoOvxT8DVVfW+oU3j0r7du54ISXYCDgOuYUzaV1Wvraq9qmo5g+/af1bVnzIm7Uty3yQ7zywDhwPfZkzat6UW1J3tSY5kcN52MfDRqnrHiKvUJMmZwO8ymL76J8CbgM8BZwPLgP8Bnl1V6w/I3+MleRJwCXAVs+fY/47BOMk4tO/RDAZjFzP4g+7sqnprkl0Zg/YN605t/XVVHT0u7UuyN4NeCAyGCD5VVe8Yl/ZtqQUVJJKkrW8hndqSJM0Dg0SS1MQgkSQ1MUgkSU0MEklSE4NEC1aSXZL85dD6Hkn+ZZ4+6xlJ3jgfx+4jyUVJJjax/ZQkT9mWddL2yyDRQrYLsC5IquqmqvqjefqsVwP/ME/Hng+nsUBmrlU7g0QL2buAfbrnSbwnyfKZZ7skOS7J55Kcl+SGJC9N8qru2RrfSPKAbr99knyxm7jvkiQPX/9DkuwH3FlVa7v1Zyf5dvcskou7ssVdHb6Z5MokfzH0/ld3z724Ism7urIVXT2uTPLZmededD2Nk7tnnVyX5He68p2SnNXt/2lgp6HP/XhXn6uSvBKgqn4A7JrkQfP1w9f4WEiTNkrrew1wQFWtgHWzDA87gMGsw/cG1gB/W1WPTXIq8HwGsyScDpxQVd9L8ngGvY71TwkdAnxraP2NwFOr6kcz06QAfw78oqoel2RHYGWSC4CHM5iK/PFVdftMgAGfAF5WVV9N8lYGsxqc2G1bUlUHdzM5vInB9CsvBm6vqkd3d9XP1GcFsGdVHdD9DGbqQ7fPIQzmO5M2yiCRNu7C7lkotyX5BXBeV34V8OhuZuLfBs4ZmpV4xw0c58HAT4fWVwIfT3I2MDMZ5eHdMWdOrS0F9mUQAh+rqtsBqupnSZYCu1TVV7t9zwDOGTr+zDFXA8u75ScDf98d48okV3bl1wN7JzkN+HfggqHj3AzssaEfjDTMIJE27s6h5emh9WkG351FwM9nejSbcAeDYACgqk7oei9HAZcnWcHgMQcvq6rzh9+Y5Ai2/HEHM/Wc4te/43c7TlXdkuQxwFOBlwB/DLyo23zvru7SJjlGooXsNmDnvm+uqluBG5I8GwYzFne/lNd3NfCwmZUk+1TVpVX1RmAtg8cbnA+8uJs6nyT7dbPKXgC8KMl9uvIHVNUvgFtmxj+APwO+yqZdDDyvO8YBwKO75d2ARVX1GeANwIFD79mPwYy20ibZI9GCVVX/m2RlN8D+H8CHehzmecCHk7yeweNyzwKuWG+fi4H3JkkNZkl9T5J9GfRCvtLtfyWD01Df6qbQ/ynwjKr6YtdjWZXkLuALDGZBfgHwkS5grgdeOEc9Pwx8rDuldTlwWVe+Z1c+80fla2Hds2AeBqza0h+IFh5n/5W2gSQfAM6rqi+Pui6bI8kxwIFV9YZR10X3fJ7akraNk4D7jLoSW2AJ8N5RV0LbB3skkqQm9kgkSU0MEklSE4NEktTEIJEkNTFIJElN/h8PZ/PQBTHllgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if plot_res:\n",
    "    fig = plt.figure()    \n",
    "    if n_speakers > 0:\n",
    "        ax1 = fig.add_subplot(111)\n",
    "    else:\n",
    "        ax1 = fig.add_subplot(211)\n",
    "    ax1.set_yticks(np.array(range(len(class_names))))\n",
    "    ax1.axis((0, duration, -1, len(class_names)))\n",
    "    ax1.set_yticklabels(class_names)\n",
    "    ax1.plot(np.array(range(len(cls)))*mt_step+mt_step/2.0, cls)\n",
    "\n",
    "if os.path.isfile(gt_file):\n",
    "    if plot_res:\n",
    "        ax1.plot(np.array(range(len(flags_gt))) *\n",
    "                 mt_step + mt_step / 2.0, flags_gt, 'r')\n",
    "    purity_cluster_m, purity_speaker_m = \\\n",
    "        evaluateSpeakerDiarization(cls, flags_gt)\n",
    "    print(\"{0:.1f}\\t{1:.1f}\".format(100 * purity_cluster_m,\n",
    "                                    100 * purity_speaker_m))\n",
    "    if plot_res:\n",
    "        plt.title(\"Cluster purity: {0:.1f}% - \"\n",
    "                  \"Speaker purity: {1:.1f}%\".format(100 * purity_cluster_m,\n",
    "                                                    100 * purity_speaker_m))\n",
    "if plot_res:\n",
    "    plt.xlabel(\"time (seconds)\")\n",
    "    #print s_range, sil_all    \n",
    "    if n_speakers<=0:\n",
    "        plt.subplot(212)\n",
    "        plt.plot(s_range, sil_all)\n",
    "        plt.xlabel(\"number of clusters\");\n",
    "        plt.ylabel(\"average clustering's sillouette\");\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['speaker0', 'speaker1']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_vec=np.array(range(len(cls)))*mt_step+mt_step/2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find Change points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_change_index=np.where(np.roll(cls,1)!=cls)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0, 132], dtype=int64)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_change_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(speaker_change_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.500000000000004"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[132]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "print(cls[135], cls[136])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'dialogue_id': '2020-02-13 15:04:18.148188',\n",
      "  'end_time': 26.300000000000004,\n",
      "  'speaker': 0.0,\n",
      "  'start_time': 0.1,\n",
      "  'text': None},\n",
      " {'dialogue_id': '2020-02-13 15:04:18.148188',\n",
      "  'end_time': 59.900000000000006,\n",
      "  'speaker': 1.0,\n",
      "  'start_time': 26.500000000000004,\n",
      "  'text': None}]\n"
     ]
    }
   ],
   "source": [
    "output_list=[]\n",
    "temp={}\n",
    "for ind,sc in enumerate(speaker_change_index):\n",
    "    temp['dialogue_id']= str(datetime.now()).strip()\n",
    "    temp['speaker']=list(cls)[sc]\n",
    "    temp['start_time']=time_vec[sc]\n",
    "    temp['end_time']=time_vec[speaker_change_index[ind+1]-1] if ind+1<len(speaker_change_index) else time_vec[-1]\n",
    "    temp[\"text\"]=None\n",
    "    output_list.append(temp)\n",
    "    temp={}\n",
    "    \n",
    "pprint.pprint(output_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wav snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_snippet(output_list,filename,output_folder):\n",
    "    for ind,diag in enumerate(output_list):\n",
    "        t1=diag['start_time']\n",
    "        t2=diag['end_time']\n",
    "        newAudio = AudioSegment.from_wav(filename)\n",
    "        newAudio = newAudio[diag['start_time']:diag['end_time']]\n",
    "        filename_out=output_folder+ f\"snippet_{str(t1)[:4]}_{str(t2)[:4]}.wav\"\n",
    "        newAudio.export(filename_out, format=\"wav\") #Exports to a wav file in the current path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_snippet(output_list,filename,\"./pyAudioAnalysis/data/Greenway/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transcribe the wav snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
